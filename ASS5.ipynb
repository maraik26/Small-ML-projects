{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: Finding rectangles\n",
    "#A nice blog-post by Johannes Rieke presents a simple setup from scratch that finds rectangles in a black & white image. \n",
    "#In order to play with it, we just have to translate a few calls from Keras to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Here is an example of the training data:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee685a5e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Create images with random rectangles and bounding boxes. \n",
    "num_imgs = 50000\n",
    "\n",
    "img_size = 8\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "num_objects = 1\n",
    "\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))  # set background to 0\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
    "        x = np.random.randint(0, img_size - w)\n",
    "        y = np.random.randint(0, img_size - h)\n",
    "        imgs[i_img, x:x+w, y:y+h] = 1.  # set rectangle to 1\n",
    "        bboxes[i_img, i_object] = [x, y, w, h]\n",
    "        \n",
    "imgs.shape, bboxes.shape\n",
    "\n",
    "display(Markdown('**Here is an example of the training data:**'))\n",
    "i = 0\n",
    "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "for bbox in bboxes[i]:\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize the image data to mean 0 and std 1. \n",
    "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)\n",
    "\n",
    "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
    "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
    "y = bboxes.reshape(num_imgs, -1) / img_size\n",
    "y.shape, np.mean(y), np.std(y)\n",
    "\n",
    "# Split training and test.\n",
    "i = int(0.8 * num_imgs)\n",
    "train_X = X[:i]\n",
    "test_X = X[i:]\n",
    "train_y = y[:i]\n",
    "test_y = y[i:]\n",
    "test_imgs = imgs[i:]\n",
    "test_bboxes = bboxes[i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1a\n",
    "#Construct a Pytorch model that resembles the Keras one in the original blog post, i.e. \n",
    "#have a fully connected, hidden layer with 200 neurons, \n",
    "#ReLU nonlinearity and dropout rate of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9d23538e5d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'parameters'"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Variable(torch.Tensor(train_X))\n",
    "labels = Variable(torch.Tensor(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-b9e90a032fd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "loss_record = []\n",
    "for epoch in range(30):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    if phase == 'train':\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    epoch_loss = running_loss / inputs.shape[0] / (epoch+1)\n",
    "    loss_record.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fee073eec50>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee68e0f4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3b4384f878ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict bounding boxes on the test images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpred_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_bboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpred_bboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not callable"
     ]
    }
   ],
   "source": [
    "# Predict bounding boxes on the test images.\n",
    "pred_y = model(Variable(torch.Tensor(test_X)))\n",
    "pred_bboxes = pred_y.data * img_size\n",
    "pred_bboxes = pred_bboxes.numpy().reshape(len(pred_bboxes), num_objects, -1)\n",
    "pred_bboxes.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
    "        return 0.\n",
    "    I = w_I * h_I\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_bboxes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-74112f1b88eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Greys'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpred_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_bbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_patch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRectangle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IOU: {:.2f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_bbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpred_bbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_bboxes' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee073b8908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a few images and predicted bounding boxes from the test dataset. \n",
    "plt.figure(figsize=(12, 3))\n",
    "for i_subplot in range(1, 5):\n",
    "    plt.subplot(1, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_imgs))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec='r', fc='none'))\n",
    "        plt.annotate('IOU: {:.2f}'.format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1]+pred_bbox[3]+0.2), color='r')\n",
    "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "summed_IOU = 0.\n",
    "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
    "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
    "mean_IOU = summed_IOU / len(pred_bboxes)\n",
    "mean_IOU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1b:\n",
    "#Move the computation that is currently done on the CPU over to the GPU using CUDA and \n",
    "#increase the number of epochs. Improve the training setup until you reach an IOU of above 0.6. \n",
    "#You can make the changes that move computation to the GPU directly in the cells above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2: Use a pretrained model\n",
    "#As mentioned in class deep learning systems are \n",
    "#hardly ever developed from scratch, but usually work \n",
    "#by refining existing solutions to similar problems. \n",
    "#For the following task, we'll work through the Transfer learning tutorial, \n",
    "#which also provides a ready-made jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the notebook and get it to run in your environment. \n",
    "# This also involves downloading the bees and ants dataset.\n",
    "# Perform your own training with the provided setup.\n",
    "# Change the currently chosen pretrained network (resnet) to \n",
    "# a different one. At least try out VGG and one other type.\n",
    "# Load a picture that you took yourself and classify it with \n",
    "# an unmodified pretrained network (e.g. the original VGG network) \n",
    "# that can detect one out of 1000 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints for step 3\n",
    "# Focus on the section Conv net as fixed feature xtractor of the transfer learning tutorial. First, change the line\n",
    "\n",
    "# model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "# to load VGG19 instead.\n",
    "\n",
    "# Next, print out the new model_conv and identify the last step of the classification. This is not named the same way as the fc layer for resnet, but works similarity. Identify the module that contains the last classification step of the VGG model, which identifies one out of 1000 classes. Change that one into identifying 2 classes only (i.e. the ants and bees that you should start with).\n",
    "\n",
    "# To change the structure of a hypothetical sequential component called module_name and modify its last layer into a DifferentLayer type, you can use this syntax.\n",
    "\n",
    "# nn.Sequential(*list(model_conv.module_name.children())[:-1] +\n",
    "#                      [nn.DifferentLayer(...)])\n",
    "# and replace the old model_conv.module_name with the differently structured version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
