{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from weather_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Row, functions, Column\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.ml import Pipeline, Estimator\n",
    "from pyspark.ml.feature import SQLTransformer, VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "from pyspark.ml.regression import (LinearRegression,\n",
    "                                   GBTRegressor,\n",
    "                                   RandomForestRegressor,\n",
    "                                   DecisionTreeRegressor)\n",
    "import sys\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model is RandomForestRegressor with validation data r2 score 0.748361\n",
      "('Best parameters on test data:\\n', ('r2', 0.6311924267636151, {Param(parent=u'RandomForestRegressor_4984baacde3597f83597', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent=u'RandomForestRegressor_4984baacde3597f83597', name='numTrees', doc='Number of trees to train (>= 1).'): 30, Param(parent=u'RandomForestRegressor_4984baacde3597f83597', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10}))\n",
      "lat_lng.png saved to local directory\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('weather related prediction').getOrCreate()\n",
    "#assert sys.version_info >= (3, 5)  # make sure we have Python 3.5+\n",
    "#assert spark.version >= '2.2'  # make sure we have Spark 2.2+\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('station', StringType(), False),\n",
    "    StructField('date', DateType(), False),\n",
    "    # StructField('dayofyear', IntegerType(), False),\n",
    "    StructField('latitude', FloatType(), False),\n",
    "    StructField('longitude', FloatType(), False),\n",
    "    StructField('elevation', FloatType(), False),\n",
    "    StructField('tmax', FloatType(), False),\n",
    "])\n",
    "\n",
    "def get_data(inputloc, tablename='data'):\n",
    "    data = spark.read.csv(inputloc, schema=schema)\n",
    "    data.createOrReplaceTempView(tablename)\n",
    "    return data\n",
    "\n",
    "def make_weather_trainers(trainRatio,\n",
    "                          estimator_gridbuilders,\n",
    "                          metricName=None):\n",
    "    \"\"\"Construct a list of TrainValidationSplit estimators for weather data\n",
    "       where `estimator_gridbuilders` is a list of (Estimator, ParamGridBuilder) tuples\n",
    "       and 0 < `trainRatio` <= 1 determines the fraction of rows used for training.\n",
    "       The RegressionEvaluator will use a non-default `metricName`, if specified.\n",
    "    \"\"\"\n",
    "    feature_cols = ['latitude', 'longitude', 'elevation', 'doy']\n",
    "    column_names = dict(featuresCol=\"features\",\n",
    "                        labelCol=\"tmax\",\n",
    "                        predictionCol=\"tmax_pred\")\n",
    "\n",
    "    spark.udf.register(\"dayofyear\", lambda x: x.timetuple().tm_yday)\n",
    "    getDOY = SQLTransformer(\n",
    "    statement=\"SELECT *, CAST (dayofyear(date) AS Integer) AS doy FROM __THIS__\")\n",
    "    # TODO: engineer a day of year feature 'doy' from schema\n",
    "\n",
    "    feature_assembler = VectorAssembler(\n",
    "        inputCols=feature_cols,\n",
    "        outputCol=column_names[\"featuresCol\"])\n",
    "    \n",
    "    ev = (RegressionEvaluator()\n",
    "          .setLabelCol(column_names[\"labelCol\"])\n",
    "          .setPredictionCol(column_names[\"predictionCol\"])\n",
    "    )\n",
    "    if metricName:\n",
    "        ev = ev.setMetricName(metricName)\n",
    "    tvs_list = []\n",
    "    for est, pgb in estimator_gridbuilders:\n",
    "        est = est.setParams(**column_names)\n",
    "\n",
    "        pl = Pipeline(stages = [getDOY, feature_assembler, est])\n",
    "        # TODO: Construct a pipeline with estimator est\n",
    "\n",
    "        paramGrid = pgb.build()\n",
    "        tvs_list.append(TrainValidationSplit(estimator=pl,\n",
    "                                             estimatorParamMaps=paramGrid,\n",
    "                                             evaluator=ev,\n",
    "                                             trainRatio=trainRatio))\n",
    "    return tvs_list\n",
    "\n",
    "def get_best_weather_model(data):\n",
    "    train, test = data.randomSplit([0.75, 0.25])\n",
    "    train = train.cache()\n",
    "    test = test.cache()\n",
    "\n",
    "    # e.g., use print(LinearRegression().explainParams()) to see what can be tuned\n",
    "    estimator_gridbuilders = [\n",
    "\n",
    "        ##to save running time, I commented out other models/parameters during tuning steps\n",
    "        #estimator_gridbuilder(\n",
    "            #LinearRegression(),\n",
    "            #dict(regParam=[0.01,0.1,0.4],         # [0.1, 0.01]\n",
    "                 #elasticNetParam=[0.1,0.5,.8,1],  # 0-L2, 1-L1\n",
    "                 #maxIter=[10]\n",
    "            #)),\n",
    "\n",
    "        #estimator_gridbuilder(\n",
    "            #GBTRegressor(),\n",
    "            #dict(maxDepth = [5,10],\n",
    "                #maxIter = [20],\n",
    "                #minInstancesPerNode = [1,5,10]\n",
    "            #)),\n",
    "\n",
    "        estimator_gridbuilder(\n",
    "            RandomForestRegressor(),\n",
    "            dict(maxDepth = [10],\n",
    "                minInstancesPerNode = [1],\n",
    "                numTrees = [30]\n",
    "            )),\n",
    "\n",
    "        #estimator_gridbuilder(\n",
    "            #DecisionTreeRegressor(),\n",
    "            #dict(maxDepth = [5,10],\n",
    "                #minInstancesPerNode = [1,5,10],\n",
    "            #))\n",
    "        \n",
    "        \n",
    "\n",
    "    ]\n",
    "    metricName = 'r2'\n",
    "    tvs_list = make_weather_trainers(.2, # fraction of data for training\n",
    "                                     estimator_gridbuilders,\n",
    "                                     metricName)\n",
    "    ev = tvs_list[0].getEvaluator()\n",
    "    scorescale = 1 if ev.isLargerBetter() else -1\n",
    "    model_name_scores = []\n",
    "    for tvs in tvs_list:\n",
    "        model = tvs.fit(train)\n",
    "        test_pred = model.transform(test)\n",
    "        score = ev.evaluate(test_pred) * scorescale\n",
    "        model_name_scores.append((model, get_estimator_name(tvs.getEstimator()), score))\n",
    "    best_model, best_name, best_score = max(model_name_scores, key=lambda triplet: triplet[2])\n",
    "    print(\"Best model is %s with validation data %s score %f\" % (best_name, ev.getMetricName(), best_score*scorescale))\n",
    "    return best_model\n",
    "\n",
    "def main():\n",
    "    data = get_data('tmax-1')\n",
    "    model = get_best_weather_model(data)\n",
    "    print(\"Best parameters on test data:\\n\", get_best_tvs_model_params(model))\n",
    "    data_pred = model.transform(data).drop(\"features\")\n",
    "    # ATTN: large file output for debugging only\n",
    "    data_pred.coalesce(1).write.csv('out1', sep=',', mode='overwrite')\n",
    "\n",
    "    hist2d(data_pred,'tmax','tmax_pred', fraction=5.e5 / data_pred.count())\n",
    "    figurename = 'pred_vs_label.png'\n",
    "    hist2d(data_pred,'longitude','latitude', fraction=5.e5 / data_pred.count())\n",
    "    figurename = 'lat_lng.png'\n",
    "    \n",
    "    plt.savefig(figurename)\n",
    "    print(figurename + ' saved to local directory')\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # Note: in current version output is only used for debugging\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
