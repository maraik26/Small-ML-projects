{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of special simbols 12\n",
      "+-----+--------------------+--------------------+\n",
      "|   id|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|   id|         rawFeatures|            features|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         1|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         2|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         2|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "99095\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|   id|         rawFeatures|            features|prediction|             score|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "|44263|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         1|               0.0|\n",
      "|44264|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         2|0.0192348792477025|\n",
      "|44265|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|               0.0|\n",
      "|44266|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         2|0.0192348792477025|\n",
      "|44267|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|               0.0|\n",
      "|44268|[udp, SF, -0.1585...|[0.0, 1.0, 0.0, 0...|         1|               0.0|\n",
      "|44269|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         5|0.8351944860012823|\n",
      "|44270|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|               0.0|\n",
      "|44271|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         2|0.0192348792477025|\n",
      "|44272|[tcp, SF, -0.1585...|[1.0, 0.0, 0.0, 0...|         1|               0.0|\n",
      "+-----+--------------------+--------------------+----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "1446 Anomalies\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|   id|         rawFeatures|            features|prediction|score|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "|44511|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44537|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44615|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44623|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44781|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44822|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44884|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44896|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|44958|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45201|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45258|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45345|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45453|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45504|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45519|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45520|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45541|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45697|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45756|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "|45915|[icmp, SF, -0.158...|[0.0, 0.0, 1.0, 0...|         3|  1.0|\n",
      "+-----+--------------------+--------------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# anomaly_detection.py\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "import operator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Anomalies Detection\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sparkCt = spark.sparkContext\n",
    "\n",
    "#convert special values which are not numbers to a vector in a form [0,1]\n",
    "def onehot(strng, indices, values, c):\n",
    "    conv = [0.0]*c # one value will be 0, c is a number of those special values\n",
    "    others = [float(strng[k]) for k in range(len(strng)) if k not in indices] #other values ina row which are not special simbols\n",
    "    for i in indices:\n",
    "        indx = values.index(Row(strng[i]))\n",
    "        conv[indx] = 1.0 # another value will be 1\n",
    "    conv.extend(others) # extend the one-hot vector with original numerical list,\n",
    "    return conv\n",
    "\n",
    "class AnomalyDetection():\n",
    "    \n",
    "    #def readToyData(self):\n",
    "    #    data = [(0, [\"http\", \"udt\", 0.4]), \\\n",
    "    #            (1, [\"http\", \"udf\", 0.5]), \\\n",
    "    #            (2, [\"http\", \"tcp\", 0.5]), \\\n",
    "    #            (3, [\"ftp\", \"icmp\", 0.1]), \\\n",
    "    #            (4, [\"http\", \"tcp\", 0.4])]\n",
    "    #    schema = [\"id\", \"rawFeatures\"]\n",
    "    #    self.rawDF = spark.createDataFrame(data, schema)\n",
    "\n",
    "    def readData(self, filename):\n",
    "        self.rawDF = spark.read.parquet(filename).cache()\n",
    "\n",
    "# in rawFeatures, the first 2 categorical data convert to one hot vector such as [0,0,1,0,1,0]\n",
    "# extend the one-hot vector with original numerical list, and all convert to Double type\n",
    "# put the numerical list to a new column called \"features\"    \n",
    "    def cat2Num(self, df, indices):\n",
    "        simbols = [] # list of special values which are not numbers\n",
    "        for i in indices:\n",
    "            d = udf(lambda r: r[i], StringType())\n",
    "            other = df.select(d(df.rawFeatures)).distinct().collect() #other numbers\n",
    "            simbols.extend(other) # extend list of simbols with others numbers\n",
    "\n",
    "        number_of_simbols = len(simbols)\n",
    "        convertUDF = udf(lambda r: onehot(r, indices, simbols, number_of_simbols), ArrayType(DoubleType()))# converted simbols plus others\n",
    "        new_dataframe = df.withColumn(\"features\", convertUDF(df.rawFeatures)) # add a new column with converted simbols plus other numbers\n",
    "        \n",
    "        print(\"number of special simbols\", len(simbols)) # number of special simbols which are not numbers (12)\n",
    "        return new_dataframe\n",
    "\n",
    "#Input: $df represents a DataFrame with four columns: \"id\", \"rawFeatures\", \"features\", and \"prediction\"\n",
    "#Output: Return a new DataFrame that adds the \"score\" column into the input $df\n",
    "#To compute the score of a data point x, we use:\n",
    "#score(x) = (N_max - N_x)/(N_max - N_min), Nmax  and  Nmin  reflect the size of the largest and smallest clusters, respectively.  \n",
    "#Nx  represents the size of the cluster assigned to  x\n",
    "#score(x)=1 when x is assigned to the smallest cluster and score(x) = 0 when x is assigned to a large cluster.\n",
    "\n",
    "    def addScore(self, df):\n",
    "        cluster_dict = {}\n",
    "        all_data = df.select(\"prediction\").collect() #take all data and add prediction column\n",
    "        \n",
    "        print(len(all_data))\n",
    "        \n",
    "        for c in all_data:\n",
    "            cluster_dict[c] = cluster_dict.setdefault(c,0.0)+1.0\n",
    "        sorted_clusters = sorted(cluster_dict.items(), key=operator.itemgetter(1))  # sort by value\n",
    "        n_max = sorted_clusters[-1][1] #maximum size of cluster\n",
    "        n_min = sorted_clusters[0][1]  #minimum size of cluster\n",
    "        score = udf(lambda p: float(n_max - cluster_dict.get(Row(p)))/(n_max - n_min), DoubleType())\n",
    "        score_dataframe = df.withColumn(\"score\", score(df.prediction)) #calculating score based on predicted clusters\n",
    "        return score_dataframe\n",
    "\n",
    "    def detect(self, k, t):\n",
    "        # Encoding categorical features using one-hot.\n",
    "        df1 = self.cat2Num(self.rawDF, [0, 1]).cache()\n",
    "        df1.show(n=5, truncate=True)\n",
    "\n",
    "        # Clustering points using KMeans\n",
    "        features = df1.select(\"features\").rdd.map(lambda row: row[0]).cache()\n",
    "        model = KMeans.train(features, k, maxIterations=40, initializationMode=\"random\", seed=60)\n",
    "\n",
    "        # Adding the prediction column to df1\n",
    "        modelBC = sparkCt.broadcast(model)\n",
    "        predictUDF = udf(lambda x: modelBC.value.predict(x), StringType())\n",
    "        df2 = df1.withColumn(\"prediction\", predictUDF(df1.features)).cache()\n",
    "        df2.show(n=5, truncate=True)\n",
    "\n",
    "        # Adding the score column to df2; The higher the score, the more likely it is an anomaly\n",
    "        df3 = self.addScore(df2).cache()\n",
    "        df3.show(n=10, truncate=True)\n",
    "\n",
    "        return df3.where(df3.score > t)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ad = AnomalyDetection()\n",
    "    \n",
    "    #ad.readToyData()\n",
    "    #anomalies = ad.detect(2, 0.9)\n",
    "    \n",
    "    ad.readData('data/logs-features-sample')\n",
    "    anomalies = ad.detect(8, 0.97)\n",
    "    \n",
    "    #seed=60                                                           seed = 20,  None\n",
    "    #K      t       Anomalies   t     Anomalies    t   Anomalies     t      Anomalies       \n",
    "    #2     0.97        1467     0.99   1467       0.9    1467 \n",
    "    #3     0.97       12902     0.99   5446       0.9   12902      0.97      453\n",
    "    #4     0.97        5433     0.99   5433       0.9   13193      0.97     1467   5433 \n",
    "    #5     0.97         467     0.99    467       0.9   13057      0.97      467  \n",
    "    #6     0.97        1446     0.99   1446       0.9    1446      0.97     1446   5479\n",
    "    #8     0.97        1889     0.99    451       0.9    1889\n",
    "    #28    0.97        2262     0.99     80       0.9   10195\n",
    "    #50    0.97        2011     0.99\n",
    "    #100   0.97        1046     0.99\n",
    "    \n",
    "    print(anomalies.count(), \"Anomalies\")\n",
    "    anomalies.show(n=20, truncate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
